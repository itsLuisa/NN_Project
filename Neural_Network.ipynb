{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the core of our project. For running this, you need to have created three accordingly formatted splits of the data by running these files from the repository:\n",
    "* preprocessing.sh\n",
    "* dataset_size.py\n",
    "* splitting_data.py\n",
    "\n",
    "You don't need to run the following files because this Notebook accesses them directly:\n",
    "* data_loading.py\n",
    "* tokenizer.py\n",
    "* embeddings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from tokenizer import tokenize_and_encode, encode_pos, initialize_tokenizer\n",
    "from embeddings import embedding_model, initialize_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Please insert the names of the three splits below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \"small_train.tsv\"\n",
    "validation_file = \"small_val.tsv\"\n",
    "test_file = \"small_test.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "(uses data_loading.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f1b68d8d23c8b204\n",
      "Reusing dataset sample (C:\\Users\\Luisa\\.cache\\huggingface\\datasets\\sample\\default-f1b68d8d23c8b204\\0.0.0\\a47856eba89262b97a9bfe6357cee71a0f0f52265edb14e72c8467526322efb5)\n"
     ]
    }
   ],
   "source": [
    "data_files = {\n",
    "            \"train\": training_file,\n",
    "            \"test\": test_file,\n",
    "            \"validation\": validation_file\n",
    "        }\n",
    "\n",
    "data_sets = datasets.load_dataset(\"data_loading.py\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data\n",
    "(uses tokenizer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   138,  7721,  1590,  2492,   170,  1490,  2730,  1107,  1134,\n",
      "          1131,   100,  1123,  1711,  1105,  7252,  1103,  7703,  1104,  1117,\n",
      "         21870,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]])}\n",
      "torch.Size([1, 73])\n",
      "{'input_ids': tensor([[101,  11,  16,  22,  42,  11,  22,  22,  15,  43,  28,  42,  29,  22,\n",
      "           9,  42,  11,  22,  15,  29,  25,   5, 102,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]])}\n",
      "torch.Size([1, 73])\n"
     ]
    }
   ],
   "source": [
    "# initialize the tokenizer\n",
    "tz = initialize_tokenizer()\n",
    "\n",
    "# tokens\n",
    "tokenized_train = tokenize_and_encode(\"train\", data_sets, tz)\n",
    "tokenized_test = tokenize_and_encode(\"test\", data_sets, tz)\n",
    "tokenized_validation = tokenize_and_encode(\"validation\", data_sets, tz)\n",
    "# lists of dictionaries with keys: \"input_ids\", \"token_type_ids\", \"attention_mask\"\n",
    "\n",
    "# pos-tags\n",
    "pos_encoded_train = encode_pos(\"train\", data_sets, tz)\n",
    "pos_encoded_test = encode_pos(\"test\", data_sets, tz)\n",
    "pos_encoded_validation = encode_pos(\"validation\", data_sets, tz)\n",
    "\n",
    "print(tokenized_train[0]) # first sentence, tokenized\n",
    "print(tokenized_train[0][\"input_ids\"].shape)\n",
    "print(pos_encoded_train[0]) # first sentence, pos, tokenized\n",
    "print(pos_encoded_train[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "(uses embeddings.py)\n",
    "\n",
    "!Attention: This might take a while!\n",
    "\n",
    "Run time on tiny.tsv: almost none <br>\n",
    "Run time on small.tsv: ~$2\\frac{1}{2}$ minutes <br>\n",
    "Run time on medium.tsv: 25 minutes <br>\n",
    "Est. Run time on big.tsv: 4 hours 10 minutes (raises run time error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0436,  0.2515, -0.2050,  ..., -0.1305,  0.3815,  0.4437],\n",
      "         [ 0.4131, -0.6782,  0.7109,  ...,  0.2691,  0.7233,  0.0436],\n",
      "         [ 0.1060, -0.4957, -0.0068,  ...,  0.2639,  0.7863,  0.1481],\n",
      "         ...,\n",
      "         [-0.3734, -0.3053, -0.1967,  ..., -0.6291,  0.6752,  0.5565],\n",
      "         [-0.4191, -0.3896, -0.2305,  ..., -0.6712,  0.6959,  0.6134],\n",
      "         [-0.4102, -0.4059, -0.1413,  ..., -0.7709,  0.7231,  0.5648]]])\n",
      "torch.Size([1, 73, 768])\n"
     ]
    }
   ],
   "source": [
    "embedded_train = list()\n",
    "embedded_val = list()\n",
    "embedded_test = list()\n",
    "\n",
    "model = initialize_model()\n",
    "for sentence in tokenized_train:\n",
    "    embedding = embedding_model(model, sentence[\"input_ids\"])\n",
    "    embedded_train.append(embedding[\"last_hidden_state\"])\n",
    "# shape of embedded sentence = [1,73,768]\n",
    "for sentence in tokenized_validation:\n",
    "    embedding = embedding_model(model, sentence[\"input_ids\"])\n",
    "    embedded_val.append(embedding[\"last_hidden_state\"])\n",
    "for sentence in tokenized_test:\n",
    "    embedding = embedding_model(model, sentence[\"input_ids\"])\n",
    "    embedded_test.append(embedding[\"last_hidden_state\"])\n",
    "\n",
    "print(embedded_train[0]) # first sentence, embedded\n",
    "print(embedded_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed throughout our trials\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# generally also changeable (but we did not do so)\n",
    "num_layers = 2\n",
    "hidden_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "# fixed\n",
    "input_size = 768\n",
    "sequence_length = 73\n",
    "num_classes = 103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "For the architecture of the network, we decided to go for an LSTM because it work well for sequence processing while at the same time handling vanishing and exploding gradients better than a regular RNN.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0,c0))\n",
    "        out = self.fc(out.view(sequence_length, -1))\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model instantiation\n",
    "In our trials we switched between the SGD-optimizer and the Adam-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch_acc, epoch_loss, eval_loss_batch):\n",
    "    for batch_idx, sentence in enumerate(tokenized_train):\n",
    "        model.zero_grad()\n",
    "        target = pos_encoded_train[batch_idx][\"input_ids\"]\n",
    "        target = target[0]\n",
    "        \n",
    "        # do embeddings\n",
    "        #embedded = embedding_model(sentence[\"input_ids\"])\n",
    "        #input_sentence = embedded[\"last_hidden_state\"]\n",
    "        input_sentence = embedded_train[batch_idx]\n",
    "        \n",
    "        # feed into model\n",
    "        scores = model(input_sentence)\n",
    "        _, predicted = torch.max(scores, 1)\n",
    "    \n",
    "        # loss\n",
    "        loss = criterion(scores, target)\n",
    "        eval_loss_batch.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # accuracy\n",
    "        right = 0\n",
    "        for n, tag in enumerate(predicted):\n",
    "            if tag == target[n]:\n",
    "                right += 1\n",
    "        acc = right / len(predicted)\n",
    "        epoch_acc += acc\n",
    "        \n",
    "        # print info\n",
    "        #print(\"batch loss:\", loss.item(), \"\\tbatch accuracy:\", acc)\n",
    "        \n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return epoch_acc, epoch_loss, eval_loss_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch_acc, epoch_loss):\n",
    "    for batch_idx, sentence in enumerate(tokenized_validation):\n",
    "        target = pos_encoded_validation[batch_idx][\"input_ids\"]\n",
    "        target = target[0]\n",
    "        \n",
    "        # do embeddings\n",
    "        #embedded = embedding_model(sentence[\"input_ids\"])\n",
    "        #input_sentence = embedded[\"last_hidden_state\"]\n",
    "        input_sentence = embedded_val[batch_idx]\n",
    "        \n",
    "        # feed into model\n",
    "        scores = model(input_sentence)\n",
    "        _, predicted = torch.max(scores, 1)\n",
    "        \n",
    "        # loss\n",
    "        loss = criterion(scores, target)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # accuracy\n",
    "        right = 0\n",
    "        for n, tag in enumerate(predicted):\n",
    "            if tag == target[n]:\n",
    "                right += 1\n",
    "        acc = right / len(predicted)\n",
    "        epoch_acc += acc  \n",
    "        \n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training and Validation\n",
    "Every dataset up to medium.tsv should generally be quite fast (depending on the number of epochs of course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EPOCH 1 ---------\n",
      "----- Epoch 1 training loss: 2.5424113641909467 training accuracy: 0.6847644503842306 -----\n",
      "----- Epoch 1 validation loss: 1.2966855553480294 validation accuracy: 0.7231822971549 -----\n",
      "--------- EPOCH 2 ---------\n",
      "----- Epoch 2 training loss: 1.2095301512901375 training accuracy: 0.7085421539146909 -----\n",
      "----- Epoch 2 validation loss: 1.0297129535904297 validation accuracy: 0.7230769230769232 -----\n",
      "--------- EPOCH 3 ---------\n",
      "----- Epoch 3 training loss: 0.9688688184369386 training accuracy: 0.7605524000445504 -----\n",
      "----- Epoch 3 validation loss: 0.8556210639958198 validation accuracy: 0.8031612223393048 -----\n",
      "--------- EPOCH 4 ---------\n",
      "----- Epoch 4 training loss: 0.8303651339942362 training accuracy: 0.812172847755877 -----\n",
      "----- Epoch 4 validation loss: 0.7495832311992462 validation accuracy: 0.8301369863013704 -----\n",
      "--------- EPOCH 5 ---------\n",
      "----- Epoch 5 training loss: 0.7267902389744191 training accuracy: 0.8405167613319993 -----\n",
      "----- Epoch 5 validation loss: 0.6513133313793402 validation accuracy: 0.8444678609062177 -----\n",
      "--------- EPOCH 6 ---------\n",
      "----- Epoch 6 training loss: 0.6226522472603777 training accuracy: 0.8571667223521571 -----\n",
      "----- Epoch 6 validation loss: 0.5566359869849223 validation accuracy: 0.8721812434141211 -----\n",
      "--------- EPOCH 7 ---------\n",
      "----- Epoch 7 training loss: 0.5299065836621979 training accuracy: 0.8752645060697204 -----\n",
      "----- Epoch 7 validation loss: 0.4764724583866505 validation accuracy: 0.8900948366701805 -----\n",
      "--------- EPOCH 8 ---------\n",
      "----- Epoch 8 training loss: 0.45329633538376507 training accuracy: 0.8910234992760906 -----\n",
      "----- Epoch 8 validation loss: 0.4102158820686432 validation accuracy: 0.9068493150684944 -----\n",
      "--------- EPOCH 9 ---------\n",
      "----- Epoch 9 training loss: 0.38950756429959604 training accuracy: 0.9086201135983979 -----\n",
      "----- Epoch 9 validation loss: 0.3566795226616355 validation accuracy: 0.9212855637513182 -----\n",
      "--------- EPOCH 10 ---------\n",
      "----- Epoch 10 training loss: 0.335896863011507 training accuracy: 0.9258826149905349 -----\n",
      "----- Epoch 10 validation loss: 0.31326502674044326 validation accuracy: 0.9342465753424668 -----\n",
      "--------- EPOCH 11 ---------\n",
      "----- Epoch 11 training loss: 0.290565017482069 training accuracy: 0.9396369306158827 -----\n",
      "----- Epoch 11 validation loss: 0.2778217018940128 validation accuracy: 0.942149631190728 -----\n",
      "--------- EPOCH 12 ---------\n",
      "----- Epoch 12 training loss: 0.25278371349128526 training accuracy: 0.950384229869697 -----\n",
      "----- Epoch 12 validation loss: 0.24906439219529813 validation accuracy: 0.9478398314014762 -----\n",
      "--------- EPOCH 13 ---------\n",
      "----- Epoch 13 training loss: 0.2212996767053941 training accuracy: 0.9571221739614665 -----\n",
      "----- Epoch 13 validation loss: 0.225470349801561 validation accuracy: 0.9533192834562706 -----\n",
      "--------- EPOCH 14 ---------\n",
      "----- Epoch 14 training loss: 0.19497293383413092 training accuracy: 0.9619668114489371 -----\n",
      "----- Epoch 14 validation loss: 0.20606674323431573 validation accuracy: 0.9577449947312968 -----\n",
      "--------- EPOCH 15 ---------\n",
      "----- Epoch 15 training loss: 0.17290117921746842 training accuracy: 0.9674796747967487 -----\n",
      "----- Epoch 15 validation loss: 0.19009756018909124 validation accuracy: 0.9608008429926245 -----\n",
      "--------- EPOCH 16 ---------\n",
      "----- Epoch 16 training loss: 0.15434736061871537 training accuracy: 0.9716004009355169 -----\n",
      "----- Epoch 16 validation loss: 0.1769551655994012 validation accuracy: 0.9623814541622767 -----\n",
      "--------- EPOCH 17 ---------\n",
      "----- Epoch 17 training loss: 0.13865906845899376 training accuracy: 0.9741062479117948 -----\n",
      "----- Epoch 17 validation loss: 0.1661001898133411 validation accuracy: 0.9645943097997898 -----\n",
      "--------- EPOCH 18 ---------\n",
      "----- Epoch 18 training loss: 0.12527278281665005 training accuracy: 0.9763893529346258 -----\n",
      "----- Epoch 18 validation loss: 0.157091064675926 validation accuracy: 0.9661749209694421 -----\n",
      "--------- EPOCH 19 ---------\n",
      "----- Epoch 19 training loss: 0.1137511378563032 training accuracy: 0.978004232097116 -----\n",
      "----- Epoch 19 validation loss: 0.1495593525146922 validation accuracy: 0.9667017913593262 -----\n",
      "--------- EPOCH 20 ---------\n",
      "----- Epoch 20 training loss: 0.10372446340932776 training accuracy: 0.9797304822363296 -----\n",
      "----- Epoch 20 validation loss: 0.14320489464399333 validation accuracy: 0.9678609062170712 -----\n",
      "--------- EPOCH 21 ---------\n",
      "----- Epoch 21 training loss: 0.09491008095541681 training accuracy: 0.9810112484686495 -----\n",
      "----- Epoch 21 validation loss: 0.1378122020405359 validation accuracy: 0.9685985247629089 -----\n",
      "--------- EPOCH 22 ---------\n",
      "----- Epoch 22 training loss: 0.08709285135108341 training accuracy: 0.9824033856776928 -----\n",
      "----- Epoch 22 validation loss: 0.13318456547478072 validation accuracy: 0.9689146469968394 -----\n",
      "--------- EPOCH 23 ---------\n",
      "----- Epoch 23 training loss: 0.08011438816263364 training accuracy: 0.9839625793518213 -----\n",
      "----- Epoch 23 validation loss: 0.12922308905085977 validation accuracy: 0.9693361433087467 -----\n",
      "--------- EPOCH 24 ---------\n",
      "----- Epoch 24 training loss: 0.0738578774034977 training accuracy: 0.9851876600957793 -----\n",
      "----- Epoch 24 validation loss: 0.12580976641844385 validation accuracy: 0.9698630136986308 -----\n",
      "--------- EPOCH 25 ---------\n",
      "----- Epoch 25 training loss: 0.06822357147579967 training accuracy: 0.9865797973048226 -----\n",
      "----- Epoch 25 validation loss: 0.12286376915872096 validation accuracy: 0.9700737618545844 -----\n",
      "--------- EPOCH 26 ---------\n",
      "----- Epoch 26 training loss: 0.06312231372235479 training accuracy: 0.9879719345138659 -----\n",
      "----- Epoch 26 validation loss: 0.12032052086522946 validation accuracy: 0.9702845100105381 -----\n",
      "--------- EPOCH 27 ---------\n",
      "----- Epoch 27 training loss: 0.058494060848294414 training accuracy: 0.9887515313509302 -----\n",
      "----- Epoch 27 validation loss: 0.1181178849674045 validation accuracy: 0.9708113804004221 -----\n",
      "--------- EPOCH 28 ---------\n",
      "----- Epoch 28 training loss: 0.054281899936466926 training accuracy: 0.9898652411181648 -----\n",
      "----- Epoch 28 validation loss: 0.11624417346185789 validation accuracy: 0.9711275026343525 -----\n",
      "--------- EPOCH 29 ---------\n",
      "----- Epoch 29 training loss: 0.05042954741000402 training accuracy: 0.9906448379552291 -----\n",
      "----- Epoch 29 validation loss: 0.11454865902017515 validation accuracy: 0.9712328767123294 -----\n",
      "--------- EPOCH 30 ---------\n",
      "----- Epoch 30 training loss: 0.04690121000204841 training accuracy: 0.9913687493039316 -----\n",
      "----- Epoch 30 validation loss: 0.113200190227228 validation accuracy: 0.9715489989462598 -----\n",
      "--------- EPOCH 31 ---------\n",
      "----- Epoch 31 training loss: 0.04366146737465453 training accuracy: 0.991702862234102 -----\n",
      "----- Epoch 31 validation loss: 0.1119756924892919 validation accuracy: 0.9716543730242366 -----\n",
      "--------- EPOCH 32 ---------\n",
      "----- Epoch 32 training loss: 0.04067921922665196 training accuracy: 0.9921483461409959 -----\n",
      "----- Epoch 32 validation loss: 0.1109831423086759 validation accuracy: 0.9721812434141207 -----\n",
      "--------- EPOCH 33 ---------\n",
      "----- Epoch 33 training loss: 0.03792837100118641 training accuracy: 0.9927052010246132 -----\n",
      "----- Epoch 33 validation loss: 0.11013051222096412 validation accuracy: 0.9721812434141207 -----\n",
      "--------- EPOCH 34 ---------\n",
      "----- Epoch 34 training loss: 0.035405645703488 training accuracy: 0.9933177413965922 -----\n",
      "----- Epoch 34 validation loss: 0.10946738586152116 validation accuracy: 0.9722866174920975 -----\n",
      "--------- EPOCH 35 ---------\n",
      "----- Epoch 35 training loss: 0.03306067422764921 training accuracy: 0.9935404833500392 -----\n",
      "----- Epoch 35 validation loss: 0.10883063621496639 validation accuracy: 0.9728134878819816 -----\n",
      "--------- EPOCH 36 ---------\n",
      "----- Epoch 36 training loss: 0.030905167609585706 training accuracy: 0.9940973382336564 -----\n",
      "----- Epoch 36 validation loss: 0.10844832240926246 validation accuracy: 0.9729188619599584 -----\n",
      "--------- EPOCH 37 ---------\n",
      "----- Epoch 37 training loss: 0.02889443859003666 training accuracy: 0.9944871366521886 -----\n",
      "----- Epoch 37 validation loss: 0.10800244534889666 validation accuracy: 0.9727081138040048 -----\n",
      "--------- EPOCH 38 ---------\n",
      "----- Epoch 38 training loss: 0.027044638008214307 training accuracy: 0.9949326205590825 -----\n",
      "----- Epoch 38 validation loss: 0.10789037214529414 validation accuracy: 0.9724973656480511 -----\n",
      "--------- EPOCH 39 ---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 39 training loss: 0.02529688984722588 training accuracy: 0.9950996770241677 -----\n",
      "----- Epoch 39 validation loss: 0.10747143945773133 validation accuracy: 0.9727081138040048 -----\n",
      "--------- EPOCH 40 ---------\n",
      "----- Epoch 40 training loss: 0.023704612759657476 training accuracy: 0.9953781044659763 -----\n",
      "----- Epoch 40 validation loss: 0.10765943963510485 validation accuracy: 0.972602739726028 -----\n",
      "--------- EPOCH 41 ---------\n",
      "----- Epoch 41 training loss: 0.0221633573798122 training accuracy: 0.9959349593495936 -----\n",
      "----- Epoch 41 validation loss: 0.10736656544389776 validation accuracy: 0.9729188619599584 -----\n",
      "--------- EPOCH 42 ---------\n",
      "----- Epoch 42 training loss: 0.02076605909027924 training accuracy: 0.9966031852099344 -----\n",
      "----- Epoch 42 validation loss: 0.10775861378866607 validation accuracy: 0.9732349841938889 -----\n",
      "--------- EPOCH 43 ---------\n",
      "----- Epoch 43 training loss: 0.0194205803114247 training accuracy: 0.996881612651743 -----\n",
      "----- Epoch 43 validation loss: 0.10751166600071324 validation accuracy: 0.973129610115912 -----\n",
      "--------- EPOCH 44 ---------\n",
      "----- Epoch 44 training loss: 0.018208748724868114 training accuracy: 0.9970486691168282 -----\n",
      "----- Epoch 44 validation loss: 0.10802992609589217 validation accuracy: 0.9729188619599584 -----\n",
      "--------- EPOCH 45 ---------\n",
      "----- Epoch 45 training loss: 0.017026970193984473 training accuracy: 0.9972714110702752 -----\n",
      "----- Epoch 45 validation loss: 0.10782427429423058 validation accuracy: 0.9734457323498424 -----\n",
      "--------- EPOCH 46 ---------\n",
      "----- Epoch 46 training loss: 0.0159668894353317 training accuracy: 0.9974941530237221 -----\n",
      "----- Epoch 46 validation loss: 0.10846554358342053 validation accuracy: 0.9733403582718657 -----\n",
      "--------- EPOCH 47 ---------\n",
      "----- Epoch 47 training loss: 0.014928192130954391 training accuracy: 0.9976612094888073 -----\n",
      "----- Epoch 47 validation loss: 0.10830482262670277 validation accuracy: 0.9736564805057961 -----\n",
      "--------- EPOCH 48 ---------\n",
      "----- Epoch 48 training loss: 0.013997386882959002 training accuracy: 0.9977725804655307 -----\n",
      "----- Epoch 48 validation loss: 0.10885171329405588 validation accuracy: 0.9734457323498424 -----\n",
      "--------- EPOCH 49 ---------\n",
      "----- Epoch 49 training loss: 0.01309398700929402 training accuracy: 0.9979396369306159 -----\n",
      "----- Epoch 49 validation loss: 0.10907117262003549 validation accuracy: 0.9735511064278193 -----\n",
      "--------- EPOCH 50 ---------\n",
      "----- Epoch 50 training loss: 0.012238909949442538 training accuracy: 0.9981066933957011 -----\n",
      "----- Epoch 50 validation loss: 0.10984847810670124 validation accuracy: 0.9730242360379352 -----\n"
     ]
    }
   ],
   "source": [
    "train_eval_loss_epoch = list()\n",
    "train_eval_loss_batch = list()\n",
    "val_eval_loss_epoch = list()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(\"--------- EPOCH\", epoch, \"---------\")\n",
    "    train_epoch_acc = 0\n",
    "    train_epoch_loss = 0\n",
    "    val_epoch_acc = 0\n",
    "    val_epoch_loss = 0\n",
    "    \n",
    "    # train\n",
    "    train_epoch_acc, train_epoch_loss, train_eval_loss_batch = train(train_epoch_acc, train_epoch_loss, train_eval_loss_batch)\n",
    "    train_epoch_loss = train_epoch_loss/len(tokenized_train)\n",
    "    train_epoch_acc = train_epoch_acc/len(tokenized_train)\n",
    "    train_eval_loss_epoch.append(train_epoch_loss)\n",
    "    \n",
    "    # validation\n",
    "    val_epoch_acc, val_epoch_loss = test(val_epoch_acc, val_epoch_loss)\n",
    "    val_epoch_loss = val_epoch_loss/len(tokenized_validation)\n",
    "    val_epoch_acc = val_epoch_acc/len(tokenized_validation)\n",
    "    val_eval_loss_epoch.append(val_epoch_loss)\n",
    "    \n",
    "    print(\"----- Epoch\", epoch, \"training loss:\", train_epoch_loss, \"training accuracy:\", train_epoch_acc, \"-----\")\n",
    "    print(\"----- Epoch\", epoch, \"validation loss:\", val_epoch_loss, \"validation accuracy:\", val_epoch_acc, \"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_total_batches = len(train_eval_loss_batch)\n",
    "\n",
    "plt.plot(train_eval_loss_epoch, label=\"train loss\")\n",
    "plt.plot(val_eval_loss_epoch, label=\"validation loss\")\n",
    "\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "\n",
    "plt.title(\"Training on the \" + training_file[:-10] + \" dataset\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"Plot_training_\" + training_file[:-10] + \"_dataset\")\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Valentina_Luisa",
   "language": "python",
   "name": "valentina_luisa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
